{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIIM_tiramisu_with_AG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rw704u-yA1K",
        "colab_type": "text"
      },
      "source": [
        "# Tiramisu with Attention Gates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeZ0dtNrgZ9u",
        "colab_type": "text"
      },
      "source": [
        "## Install dependencies and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7gsu-kmmD3v",
        "colab_type": "code",
        "outputId": "083e1bd3-d6f9-4805-eb87-f2227d22e4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u92getfvZrFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tqdm -U\n",
        "!pip install nibabel\n",
        "!pip install h5py\n",
        "!pip install dominate\n",
        "!pip install pydicom\n",
        "!pip install https://github.com/ozan-oktay/torchsample/tarball/master#egg=torchsample-0.1.3\n",
        "!pip install git+https://github.com/baldassarreFe/pytorch-densenet-tiramisu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXBjENLveAnP",
        "colab_type": "code",
        "outputId": "49293eca-0b2d-454d-f7cb-2647467157bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import json\n",
        "import collections\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import numbers\n",
        "import PIL\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.3.1\n",
            "Torchvision Version:  0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_3QqLS-V6yC",
        "colab_type": "text"
      },
      "source": [
        "## Setup dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGduGTl4WqGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# local machine directories\n",
        "DATA_DIR = \"/content/data/\"\n",
        "DATA_MASKS_DIR = DATA_DIR + \"masks\"\n",
        "DATA_DICOM_DIR = DATA_DIR + \"input\"\n",
        "\n",
        "# google drive directories\n",
        "DATA_ZIP_FILE_PATH = \"/content/gdrive/My Drive/mlmi/dataset/input.zip\"\n",
        "MASKS_DIR = \"/content/gdrive/My Drive/mlmi/dataset/masks/\"\n",
        "TENSORBOARD_LOGS_DIR = \"/content/gdrive/My Drive/mlmi/results/tensorboard/\"\n",
        "MODEL_SAVE_DIR = \"/content/gdrive/My Drive/mlmi/results/model/\"\n",
        "IMG_RESULT_SAVE_DIR = r'/content/gdrive/My Drive/mlmi/results/imgs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XITnSUqxZkXe",
        "colab_type": "text"
      },
      "source": [
        "Copy masks from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yof8muQ0sdOs",
        "colab_type": "code",
        "outputId": "539b1e13-8240-4fa6-f62f-931c912941e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# create data directory locally\n",
        "if os.path.exists(DATA_DIR) == False:\n",
        "  os.makedirs(DATA_DIR)\n",
        "  print(\"Create directory: \" + DATA_DIR)\n",
        "\n",
        "if os.path.exists(DATA_MASKS_DIR):\n",
        "  shutil.rmtree(DATA_MASKS_DIR)\n",
        "  print(\"Deleted old masks directory: \" + DATA_MASKS_DIR)\n",
        "\n",
        "!cp -r \"$MASKS_DIR\" $DATA_DIR\n",
        "%ls $DATA_MASKS_DIR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleted old masks directory: /content/data/masks\n",
            "simm_DS_test.csv  simm_DS_train.csv  simm_DS_validation.csv  train-rle.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_wBRjKYXuFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## uncomment this code to reduce the train set size\n",
        "# includeImagesCounts = 50\n",
        "# trainSetCsv = pd.read_csv(DATA_MASKS_DIR + '/simm_DS_train.csv')\n",
        "# tsReduced = trainSetCsv[:includeImagesCounts]\n",
        "# tsReduced[:5]\n",
        "\n",
        "# tsReduced.to_csv(DATA_MASKS_DIR + '/simm_DS_train.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBzkrPwNfdZ_",
        "colab_type": "text"
      },
      "source": [
        "Extract DICOM files from zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg8ThUO3mJhT",
        "colab_type": "code",
        "outputId": "0b9a7a9a-110d-450f-8399-7279107f8034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# only extract if input folder doesn't exists\n",
        "if os.path.exists(DATA_DICOM_DIR) == False:\n",
        "  #!unzip \"$DATA_ZIP_FILE_PATH\" -d \"$DATA_DIR\"\n",
        "  zip_ref = zipfile.ZipFile(DATA_ZIP_FILE_PATH, 'r')\n",
        "  zip_ref.extractall(DATA_DIR)\n",
        "  zip_ref.close()\n",
        "  print(\"Extracted dicom zip file to directory: \" + DATA_DICOM_DIR)\n",
        "\n",
        "%ls $DATA_DICOM_DIR\n",
        "#%ls \"$DATA_DICOM_DIR/siim/dicom-images-train\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msiim\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6dCJ9tQhEyJ",
        "colab_type": "text"
      },
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq628aADYjqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width* height)\n",
        "    array = np.asarray([int(x) for x in rle.split()])\n",
        "    starts = array[0::2]\n",
        "    lengths = array[1::2]\n",
        "\n",
        "    current_position = 0\n",
        "    for index, start in enumerate(starts):\n",
        "        current_position += start\n",
        "        mask[current_position:current_position+lengths[index]] = 255\n",
        "        current_position += lengths[index]\n",
        "\n",
        "    return mask.reshape(width, height)\n",
        "    \n",
        "class SIMMDataset(Dataset):\n",
        "    \"\"\"SIMM dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, split, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dicomPaths (Array<string>): Array of DICOM file Paths.\n",
        "            mask_csv_file (string): csv file with encoded masks (rle).\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.dir_postfix = 'input/siim/'\n",
        "\n",
        "        self.im_height = 1024\n",
        "        self.im_width = 1024\n",
        "        self.im_chan = 1\n",
        "\n",
        "        ## Read masks file\n",
        "        mask_csv_file = root_dir + 'masks/train-rle.csv' \n",
        "        print(\"Reading masks from: \" + mask_csv_file)\n",
        "        self.encodedMasks = pd.read_csv(mask_csv_file, names=['ImageId', 'EncodedPixels'], index_col='ImageId')\n",
        "\n",
        "        ## Read dataset file names\n",
        "        dsFile = root_dir + 'masks/simm_DS_' + split + '.csv'\n",
        "        print(\"Reading ds from: \" + dsFile)\n",
        "        dsFileData = pd.read_csv(dsFile)\n",
        "        self.dicomPaths = dsFileData['path'].tolist()\n",
        "        # print(\"READ\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dicomPaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Read dicom file\n",
        "        dPath = self.dicomPaths[idx]\n",
        "        dicom = pydicom.dcmread(self.root_dir + self.dir_postfix + dPath)\n",
        "        \n",
        "        # get image from dicom to numpy array\n",
        "        image = np.array(dicom.pixel_array)\n",
        "        \n",
        "        # get mask (in rle) from csv\n",
        "        landmarks = np.zeros((self.im_height, self.im_width), dtype=np.bool)\n",
        "        \n",
        "        fileId = dPath.split('/')[-1][:-4]\n",
        "        rle = self.encodedMasks.loc[fileId, 'EncodedPixels']\n",
        "        try:\n",
        "            if type(rle) == str: # if single rle\n",
        "                decodedRle = rle2mask(rle, self.im_height, self.im_width)\n",
        "#                 landmarks = np.expand_dims(decodedRle, axis=2)\n",
        "                landmarks = decodedRle\n",
        "            else: # if multiple rle\n",
        "                for x in rle:\n",
        "                    decodedRle = rle2mask(x, self.im_height, self.im_width)\n",
        "                    landmarks = landmarks + decodedRle\n",
        "#                     landmarks = landmarks + np.expand_dims(decodedRle, axis=2)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            \n",
        "        ## BECAUSE WHEN PLOTING THE GRAPHS WE HAVE TO TRANSPOSE IT.\n",
        "        landmarks = landmarks.T\n",
        "\n",
        "        # for some images, we have multiple masks, so we are adding the masks\n",
        "        # which results in some pixels to > 1\n",
        "        landmarks = (landmarks >= 1).astype('float32')\n",
        "\n",
        "        image = np.array(image)\n",
        "        landmarks = np.array(landmarks)\n",
        "\n",
        "        # converting numpy array to PIL image\n",
        "        image = PIL.Image.fromarray(image)\n",
        "        landmarks = PIL.Image.fromarray(landmarks)\n",
        "\n",
        "        size = (224, 224)\n",
        "        image = image.resize(size, PIL.Image.BILINEAR)\n",
        "        landmarks = landmarks.resize(size, PIL.Image.BILINEAR)\n",
        "\n",
        "        if self.transform:\n",
        "          # torchvision transforms needs PIL image\n",
        "          # apply transformation\n",
        "          image, landmarks = self.transform(image, landmarks)\n",
        "\n",
        "        # convert image to 3 channel\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        # converting back to numpy array from PIL image\n",
        "        image = np.array(image)\n",
        "        landmarks = np.array(landmarks)\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # but torch image: C X H X W\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "        landmarks = np.expand_dims(landmarks, axis=0)\n",
        "        image = image / 255\n",
        "\n",
        "        return image, landmarks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOVtBvQCtDqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridAttentionBlock2D(nn.Module):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=2, mode='concatenation', sub_sample_factor=(2,2)):\n",
        "        super(GridAttentionBlock2D, self).__init__()\n",
        "\n",
        "        assert dimension in [2]\n",
        "        assert mode in ['concatenation']\n",
        "\n",
        "        # Downsampling rate for the input featuremap\n",
        "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
        "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
        "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
        "\n",
        "        # Default parameter set\n",
        "        self.mode = mode\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
        "\n",
        "        # Number of channels (pixel dimensions)\n",
        "        self.in_channels = in_channels\n",
        "        self.gating_channels = gating_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            bn = nn.BatchNorm2d\n",
        "            self.upsample_mode = 'bilinear'\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # Output transform\n",
        "        self.W = nn.Sequential(\n",
        "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
        "            bn(self.in_channels),\n",
        "        )\n",
        "\n",
        "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        # Initialise weights\n",
        "        # for m in self.children():\n",
        "        #     init_weights(m, init_type='kaiming')\n",
        "\n",
        "        # Define the operation\n",
        "        if mode == 'concatenation':\n",
        "            self.operation_function = self._concatenation\n",
        "        else:\n",
        "            raise NotImplementedError('Unknown operation function.')\n",
        "\n",
        "\n",
        "    def forward(self, input, gating_signal):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :param g: (b, g_d)\n",
        "        :return:\n",
        "        '''\n",
        "        output = self.operation_function(input, gating_signal)\n",
        "        return output\n",
        "\n",
        "    def _concatenation(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.interpolate(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.relu(theta_x + phi_g, inplace=True)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        sigm_psi_f = torch.sigmoid(self.psi(f))\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.interpolate(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        # gate, attention\n",
        "        return W_y, sigm_psi_f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqMnprX7aF2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dense import DenseLayer, DenseBlock, TransitionUp, TransitionDown\n",
        "from torch.nn import Module, Conv2d, BatchNorm2d, Linear, init\n",
        "from torch.nn import Sequential\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "## https://github.com/baldassarreFe/pytorch-densenet-tiramisu\n",
        "class FCDenseNet(Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 out_channels: int = 1,\n",
        "                 initial_num_features: int = 48,\n",
        "                 dropout: float = 0.2,\n",
        "\n",
        "                 down_dense_growth_rates: Union[int, Sequence[int]] = 16,\n",
        "                 down_dense_bottleneck_ratios: Union[Optional[int], Sequence[Optional[int]]] = None,\n",
        "                 down_dense_num_layers: Union[int, Sequence[int]] = (4, 5, 7, 10, 12),\n",
        "                 down_transition_compression_factors: Union[float, Sequence[float]] = 1.0,\n",
        "\n",
        "                 middle_dense_growth_rate: int = 16,\n",
        "                 middle_dense_bottleneck: Optional[int] = None,\n",
        "                 middle_dense_num_layers: int = 15,\n",
        "\n",
        "                 up_dense_growth_rates: Union[int, Sequence[int]] = 16,\n",
        "                 up_dense_bottleneck_ratios: Union[Optional[int], Sequence[Optional[int]]] = None,\n",
        "                 up_dense_num_layers: Union[int, Sequence[int]] = (12, 10, 7, 5, 4)):\n",
        "      \n",
        "        super(FCDenseNet, self).__init__()\n",
        "\n",
        "        # region Parameters handling\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if type(down_dense_growth_rates) == int:\n",
        "            down_dense_growth_rates = (down_dense_growth_rates,) * 5\n",
        "        if down_dense_bottleneck_ratios is None or type(down_dense_bottleneck_ratios) == int:\n",
        "            down_dense_bottleneck_ratios = (down_dense_bottleneck_ratios,) * 5\n",
        "        if type(down_dense_num_layers) == int:\n",
        "            down_dense_num_layers = (down_dense_num_layers,) * 5\n",
        "        if type(down_transition_compression_factors) == float:\n",
        "            down_transition_compression_factors = (down_transition_compression_factors,) * 5\n",
        "\n",
        "        if type(up_dense_growth_rates) == int:\n",
        "            up_dense_growth_rates = (up_dense_growth_rates,) * 5\n",
        "        if up_dense_bottleneck_ratios is None or type(up_dense_bottleneck_ratios) == int:\n",
        "            up_dense_bottleneck_ratios = (up_dense_bottleneck_ratios,) * 5\n",
        "        if type(up_dense_num_layers) == int:\n",
        "            up_dense_num_layers = (up_dense_num_layers,) * 5\n",
        "        # endregion\n",
        "\n",
        "        # region First convolution\n",
        "        self.features = Conv2d(in_channels, initial_num_features, kernel_size=3, padding=1, bias=False)\n",
        "        current_channels = self.features.out_channels\n",
        "        # endregion\n",
        "\n",
        "        # region Downward path\n",
        "        # Pairs of Dense Blocks with input concatenation and TransitionDown layers\n",
        "        down_dense_params = [\n",
        "            {\n",
        "                'concat_input': True,\n",
        "                'growth_rate': gr,\n",
        "                'num_layers': nl,\n",
        "                'dense_layer_params': {\n",
        "                    'dropout': dropout,\n",
        "                    'bottleneck_ratio': br\n",
        "                }\n",
        "            }\n",
        "            for gr, nl, br in\n",
        "            zip(down_dense_growth_rates, down_dense_num_layers, down_dense_bottleneck_ratios)\n",
        "        ]\n",
        "\n",
        "        down_transition_params = [\n",
        "            {\n",
        "                'dropout': dropout,\n",
        "                'compression': c\n",
        "            } for c in down_transition_compression_factors\n",
        "        ]\n",
        "\n",
        "        skip_connections_channels = []\n",
        "\n",
        "        self.down_dense = Module()\n",
        "        self.down_trans = Module()\n",
        "\n",
        "        down_pairs_params = zip(down_dense_params, down_transition_params)\n",
        "        for i, (dense_params, transition_params) in enumerate(down_pairs_params):\n",
        "            block = DenseBlock(current_channels, **dense_params)\n",
        "            current_channels = block.out_channels\n",
        "            self.down_dense.add_module(f'block_{i}', block)\n",
        "\n",
        "            skip_connections_channels.append(block.out_channels)\n",
        "\n",
        "            transition = TransitionDown(current_channels, **transition_params)\n",
        "            current_channels = transition.out_channels\n",
        "            self.down_trans.add_module(f'trans_{i}', transition)\n",
        "        # endregion\n",
        "\n",
        "        # region Middle block\n",
        "        # Renamed from \"bottleneck\" in the paper, to avoid confusion with the Bottleneck of DenseLayers\n",
        "        self.middle = DenseBlock(\n",
        "            current_channels,\n",
        "            middle_dense_growth_rate,\n",
        "            middle_dense_num_layers,\n",
        "            concat_input=True,\n",
        "            dense_layer_params={\n",
        "                'dropout': dropout,\n",
        "                'bottleneck_ratio': middle_dense_bottleneck\n",
        "            })\n",
        "        current_channels = self.middle.out_channels\n",
        "        # endregion\n",
        "\n",
        "        # region Attention Gate\n",
        "        \n",
        "        # endregion\n",
        "\n",
        "        # region Upward path\n",
        "        # Pairs of TransitionUp layers and Dense Blocks without input concatenation\n",
        "        up_transition_params = [\n",
        "            {\n",
        "                'skip_channels': sc,\n",
        "            } for sc in reversed(skip_connections_channels)\n",
        "        ]\n",
        "        up_dense_params = [\n",
        "            {\n",
        "                'concat_input': False,\n",
        "                'growth_rate': gr,\n",
        "                'num_layers': nl,\n",
        "                'dense_layer_params': {\n",
        "                    'dropout': dropout,\n",
        "                    'bottleneck_ratio': br\n",
        "                }\n",
        "            }\n",
        "            for gr, nl, br in\n",
        "            zip(up_dense_growth_rates, up_dense_num_layers, up_dense_bottleneck_ratios)\n",
        "        ]\n",
        "\n",
        "        self.up_dense = Module()\n",
        "        self.up_trans = Module()\n",
        "        self.attention_blocks = Module()\n",
        "        up_pairs_params = zip(up_transition_params, up_dense_params)\n",
        "        for i, (transition_params_up, dense_params_up) in enumerate(up_pairs_params):\n",
        "            ag_in = transition_params_up['skip_channels']\n",
        "            attention_gate = GridAttentionBlock2D(in_channels=ag_in, \n",
        "                                                  gating_channels=current_channels, \n",
        "                                                  inter_channels=ag_in, \n",
        "                                                  dimension=2, \n",
        "                                                  mode='concatenation', \n",
        "                                                  sub_sample_factor=(2,2))\n",
        "            self.attention_blocks.add_module(f'attention_{i}', attention_gate)\n",
        "            # print(transition_params_up['skip_channels'])\n",
        "\n",
        "            transition = TransitionUp(current_channels, **transition_params_up)\n",
        "            current_channels = transition.out_channels\n",
        "            self.up_trans.add_module(f'trans_{i}', transition)\n",
        "\n",
        "            block = DenseBlock(current_channels, **dense_params_up)\n",
        "            current_channels = block.out_channels\n",
        "            self.up_dense.add_module(f'block_{i}', block)\n",
        "        # endregion\n",
        "\n",
        "        # region Final convolution\n",
        "        self.final = Conv2d(current_channels, out_channels, kernel_size=1, bias=False)\n",
        "        # endregion\n",
        "\n",
        "        self.last_attention_coefficients = []\n",
        "\n",
        "        # region Weight initialization\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, Conv2d):\n",
        "                init.kaiming_normal_(module.weight)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                module.reset_parameters()\n",
        "            elif isinstance(module, Linear):\n",
        "                init.xavier_uniform_(module.weight)\n",
        "                init.constant_(module.bias, 0)\n",
        "        # endregion\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.features(x)\n",
        "\n",
        "        skip_tensors = []\n",
        "        for dense, trans in zip(self.down_dense.children(), self.down_trans.children()):\n",
        "            res = dense(res)\n",
        "            skip_tensors.append(res)\n",
        "            res = trans(res)\n",
        "\n",
        "        res = self.middle(res)\n",
        "\n",
        "        attention_coefficients = []\n",
        "\n",
        "        for skip, attention_block, trans, dense in zip(reversed(skip_tensors), self.attention_blocks.children(), self.up_trans.children(), self.up_dense.children()):\n",
        "            gate, att_cof = attention_block(skip, res)\n",
        "\n",
        "            attention_coefficients.append(att_cof)\n",
        "\n",
        "            res = trans(res, gate)\n",
        "            res = dense(res)\n",
        "\n",
        "        res = self.final(res)\n",
        "\n",
        "        self.last_attention_coefficients = attention_coefficients\n",
        "\n",
        "        return torch.sigmoid(res)\n",
        "\n",
        "    def getLastAttentionCoefficients(self):\n",
        "        return self.last_attention_coefficients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DXmfSVodOwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model to use\n",
        "class FCDenseNet103(FCDenseNet):\n",
        "    def __init__(self, in_channels=3, out_channels=1000, dropout=0.0):\n",
        "        super(FCDenseNet103, self).__init__(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            initial_num_features=48,\n",
        "            dropout=dropout,\n",
        "\n",
        "            down_dense_growth_rates=16,\n",
        "            down_dense_bottleneck_ratios=None,\n",
        "            down_dense_num_layers=(4, 5, 7, 10, 12),\n",
        "            down_transition_compression_factors=1.0,\n",
        "\n",
        "            middle_dense_growth_rate=16,\n",
        "            middle_dense_bottleneck=None,\n",
        "            middle_dense_num_layers=15,\n",
        "\n",
        "            up_dense_growth_rates=16,\n",
        "            up_dense_bottleneck_ratios=None,\n",
        "            up_dense_num_layers=(12, 10, 7, 5, 4)\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOuDOXyMaAtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SIMMSoftDiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIMMSoftDiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        smooth = 0.01\n",
        "        batch_size = input.size(0)\n",
        "\n",
        "        # input = torch.sigmoid(input).view(batch_size, -1)\n",
        "        input = input.view(batch_size, -1)\n",
        "        labels = target.contiguous().view(batch_size, -1)\n",
        "\n",
        "        inter = torch.sum(input * labels, 1) + smooth\n",
        "        union = torch.sum(input, 1) + torch.sum(labels, 1) + smooth\n",
        "\n",
        "        score = torch.sum(2.0 * inter / union, 0)\n",
        "        score = 1.0 - score / float(batch_size)\n",
        "        \n",
        "        return score\n",
        "\n",
        "class SIMMHardDiceScore(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIMMHardDiceScore, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        smooth = 0.01\n",
        "\n",
        "        # convert input to hard classification\n",
        "        hardClf = input.clone()\n",
        "        hardClf[hardClf > 0.5] = 1\n",
        "        hardClf[hardClf <= 0.5] = 0\n",
        "\n",
        "        labels = target.clone()\n",
        "        batch_size = hardClf.size(0)\n",
        "\n",
        "        # input = torch.sigmoid(input).view(batch_size, -1)\n",
        "        hardClf = hardClf.view(batch_size, -1)\n",
        "        labels = labels.contiguous().view(batch_size, -1)\n",
        "\n",
        "        inter = torch.sum(hardClf * labels, 1) + smooth\n",
        "        union = torch.sum(hardClf, 1) + torch.sum(labels, 1) + smooth\n",
        "\n",
        "        score = torch.sum(2.0 * inter / union, 0)\n",
        "        score = score / float(batch_size)\n",
        "        \n",
        "        return score\n",
        "\n",
        "class SIMMCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIMMCrossEntropyLoss, self).__init__()\n",
        "        self.BCE = nn.BCELoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        t = target.float()\n",
        "        return self.BCE(input, t)\n",
        "\n",
        "class SIMMDiceAndEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIMMDiceAndEntropyLoss, self).__init__()\n",
        "        self.diceLoss = SIMMSoftDiceLoss()\n",
        "        self.BCE = SIMMCrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        bceLoss = self.BCE(input, target)\n",
        "        dLoss = self.diceLoss(input, target)\n",
        "\n",
        "        combinedLoss = 0.5 * bceLoss + 0.5 * dLoss\n",
        "        \n",
        "        return combinedLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyWrBfFyLM-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def json_file_to_pyobj(jsonStr):\n",
        "    def _json_object_hook(d): return collections.namedtuple('X', d.keys())(*d.values())\n",
        "    def json2obj(data): return json.loads(data, object_hook=_json_object_hook)\n",
        "    return json2obj(jsonStr)\n",
        "    \n",
        "def create_checkpoint(run_name, model, optimizer, is_best, epoch, network_options, train_loss, valid_loss, best_valid_loss):\n",
        "\n",
        "  save_dir = os.path.join(MODEL_SAVE_DIR, run_name)\n",
        "  if os.path.exists(save_dir) == False:\n",
        "    os.makedirs(save_dir)\n",
        "    print('Creating save dir: ' + save_dir)\n",
        "\n",
        "  print('Saving model checkpoint at epoch: ' + str(epoch))\n",
        "  state = {\n",
        "      'run_name': run_name,\n",
        "      'model_state': model.state_dict(),\n",
        "      'optimizer_state': optimizer.state_dict(),\n",
        "      'epoch': epoch,\n",
        "      'rng_state': torch.get_rng_state(),\n",
        "      'network_options': network_options,\n",
        "      'train_loss': train_loss,\n",
        "      'valid_loss': valid_loss,\n",
        "      'best_valid_loss': best_valid_loss\n",
        "  }\n",
        "\n",
        "  torch.save(state, os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint.pt'))\n",
        "  if is_best:\n",
        "    torch.save(state, os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint_best.pt'))\n",
        "\n",
        "def load_checkpoint(run_name, model, optimizer, load_best=False):\n",
        "  ckh_path = os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint.pt')\n",
        "  if load_best:\n",
        "    ckh_path = os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint_best.pt')\n",
        "  checkpoint = torch.load(ckh_path)\n",
        "  model.load_state_dict(checkpoint['model_state'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "  torch.set_rng_state(checkpoint['rng_state'])\n",
        "\n",
        "  return model, optimizer, checkpoint\n",
        "\n",
        "def print_checkpoint(checkpoint):\n",
        "  print(\"Run Name: \" + str(checkpoint['run_name']))\n",
        "  print(\"Last epoch: \" + str(checkpoint['epoch']))\n",
        "  print(\"Train Loss: \" + str(checkpoint['train_loss']))\n",
        "  print(\"Validation Loss: \" + str(checkpoint['valid_loss']))\n",
        "  print(\"Best Validation Loss: \" + str(checkpoint['best_valid_loss']))\n",
        "  print(\"Options: \" + str(checkpoint['network_options']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD7YUvq0tLnR",
        "colab_type": "text"
      },
      "source": [
        "Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1RkAFHbtKWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SegCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class SegRandomAffine(object):\n",
        "  def __init__(self, degrees, translate=None, scale=None, resample=False, fillcolor=0):\n",
        "    # NOTE: shear is REMOVED FOR NOW\n",
        "\n",
        "    # just to check the values using this contructor\n",
        "    self.segTransform = T.RandomAffine(degrees, translate, scale, None, resample, fillcolor)\n",
        "\n",
        "    if isinstance(degrees, numbers.Number):\n",
        "        self.degrees = (-degrees, degrees)\n",
        "    else:\n",
        "        self.degrees = degrees\n",
        "        \n",
        "    self.translate = translate\n",
        "    self.scale = scale\n",
        "    self.resample = resample\n",
        "    self.fillcolor = fillcolor\n",
        "\n",
        "  def __call__(self, image, target):\n",
        "        ret = T.RandomAffine.get_params(self.degrees, self.translate, self.scale, None, image.size)\n",
        "        image = T.functional.affine(image, *ret, resample=self.resample, fillcolor=self.fillcolor)\n",
        "        target = T.functional.affine(target, *ret, resample=self.resample, fillcolor=self.fillcolor)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "class SegToTensor(object):\n",
        "  def __init__(self):\n",
        "    self.ToTensor = T.ToTensor()\n",
        "  def __call__(self, image, target):\n",
        "        image = self.ToTensor(image)\n",
        "        target = self.ToTensor(target)\n",
        "\n",
        "        return image, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6gvdAuEhtzH",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuFU4n1AXaG_",
        "colab_type": "code",
        "outputId": "4f3dd0ab-e287-49e5-ccca-46526366859f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "use_cuda = False\n",
        "if torch.cuda.is_available():\n",
        "  gpu_count = torch.cuda.device_count()\n",
        "  print(\"Available GPU count:\" + str(gpu_count))\n",
        "  use_cuda = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available GPU count:1\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-M4zJvJh1v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model Hyperparameters\n",
        "\n",
        "NETWORK_OPTIONS = dict()\n",
        "NETWORK_OPTIONS['USE_CUDA'] = use_cuda\n",
        "\n",
        "# Set to false if only need to run on train dataset\n",
        "NETWORK_OPTIONS['USE_VAL_SET'] = True\n",
        "\n",
        "# NETWORK_OPTIONS['LR_POLICY'] = \"step\"\n",
        "# NETWORK_OPTIONS['LR_DECAY_ITERS'] = 25\n",
        "NETWORK_OPTIONS['LR_RATE'] = 1e-5\n",
        "NETWORK_OPTIONS['L2_REG_WEIGHT'] = 1e-6\n",
        "NETWORK_OPTIONS['DROPOUT'] = 0.2\n",
        "\n",
        "NETWORK_OPTIONS['BATCH_SIZE'] = 8\n",
        "\n",
        "# dont forget to set run_name when continue_train is true\n",
        "NETWORK_OPTIONS['CONTINUE_TRAIN'] = False\n",
        "NETWORK_OPTIONS['LOAD_BEST'] = False\n",
        "NETWORK_OPTIONS['RUN_NAME'] = '' # If CONTINUE_TRAIN, then provide name of run to continue\n",
        "NETWORK_OPTIONS['START_EPOCH'] = 0\n",
        "NETWORK_OPTIONS['NUM_EPOCHS'] = 300\n",
        "\n",
        "# NETWORK_OPTIONS['FEATURE_SCALE'] = 2\n",
        "# NETWORK_OPTIONS['DIVISION_FACTOR'] = 1\n",
        "\n",
        "NETWORK_OPTIONS['DATALOADER_NUM_WORKERS'] = 1\n",
        "NETWORK_OPTIONS['DATA_DIR'] = DATA_DIR\n",
        "NETWORK_OPTIONS['TENSORBOARD_LOGS_DIR'] = TENSORBOARD_LOGS_DIR\n",
        "NETWORK_OPTIONS['MODEL_SAVE_DIR'] = MODEL_SAVE_DIR\n",
        "\n",
        "# converting dict() to python object (just for easy access)\n",
        "NETWORK_OPTIONS_OBJ = json_file_to_pyobj(json.dumps(NETWORK_OPTIONS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SNziec4bb8n",
        "colab_type": "code",
        "outputId": "ff4497d7-b4d4-4707-b036-03802333f652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Defining Dataloaders\n",
        "\n",
        "rotate_val = 20.0\n",
        "translation_val = [0.1,0.1]\n",
        "scale_val = [0.7,1.3]\n",
        "\n",
        "train_transform = SegCompose([\n",
        "                              SegRandomAffine(degrees=rotate_val, translate=translation_val, scale=scale_val)\n",
        "                             ])\n",
        "train_transform = None\n",
        "\n",
        "train_dataset = SIMMDataset(NETWORK_OPTIONS_OBJ.DATA_DIR, split='train', transform=train_transform)\n",
        "valid_dataset = SIMMDataset(NETWORK_OPTIONS_OBJ.DATA_DIR, split='validation', transform=None)\n",
        "test_dataset  = SIMMDataset(NETWORK_OPTIONS_OBJ.DATA_DIR, split='test', transform=None)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading masks from: /content/data/masks/train-rle.csv\n",
            "Reading ds from: /content/data/masks/simm_DS_train.csv\n",
            "Reading masks from: /content/data/masks/train-rle.csv\n",
            "Reading ds from: /content/data/masks/simm_DS_validation.csv\n",
            "Reading masks from: /content/data/masks/train-rle.csv\n",
            "Reading ds from: /content/data/masks/simm_DS_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf0Fb21iPNPJ",
        "colab_type": "code",
        "outputId": "96b724a5-ba47-4303-e94f-92337f9c67b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Size of train set: ', len(train_dataset))\n",
        "print('Size of validation set: ', len(valid_dataset))\n",
        "print('Size of test set: ', len(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train set:  1600\n",
            "Size of validation set:  532\n",
            "Size of test set:  532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEsBmffec2SS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FCDenseNet103(in_channels=3,\n",
        "                      out_channels=1,\n",
        "                      dropout=NETWORK_OPTIONS_OBJ.DROPOUT)\n",
        "# print(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=NETWORK_OPTIONS_OBJ.LR_RATE,\n",
        "                        betas=(0.9, 0.999),\n",
        "                        weight_decay=NETWORK_OPTIONS_OBJ.L2_REG_WEIGHT)\n",
        "\n",
        "# optimizer = optim.SGD(params,\n",
        "#                               lr=option.lr_rate,\n",
        "#                               momentum=0.9,\n",
        "#                               nesterov=True,\n",
        "#                               weight_decay=option.l2_reg_weight)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K85NkkrxGDWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Only for transfer learning\n",
        "## use this code to load weights from pytorch state file\n",
        "# checkpoint_group1 = torch.load('/content/checkpoint-tiramisu-model-state')\n",
        "# other_model_state = checkpoint_group1['model_state']\n",
        "# model.load_state_dict(other_model_state, strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lakj338zM-r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_best_valid_loss = 100.0\n",
        "if NETWORK_OPTIONS_OBJ.CONTINUE_TRAIN == True:\n",
        "  run_name = NETWORK_OPTIONS_OBJ.RUN_NAME\n",
        "  model, optimizer, checkpoint = load_checkpoint(run_name, model, optimizer, load_best=NETWORK_OPTIONS_OBJ.LOAD_BEST)\n",
        "  last_best_valid_loss = checkpoint['best_valid_loss']\n",
        "  print_checkpoint(checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6B2Wa5hGICt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_images_with_masks_and_prediction(images, labels, predictions):\n",
        "  # create grid of images\n",
        "  img_grid = torchvision.utils.make_grid(images.clone().cpu())\n",
        "  target_grid = torchvision.utils.make_grid(labels.clone().cpu())\n",
        "  prediction_grid = torchvision.utils.make_grid(predictions.clone().cpu())\n",
        "  # print(img_grid.shape)\n",
        "\n",
        "  img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "  target_grid = np.transpose(target_grid.numpy(), (1, 2, 0))\n",
        "  prediction_grid = np.transpose(prediction_grid.numpy(), (1, 2, 0))\n",
        "\n",
        "\n",
        "  mask_target = (target_grid == [1.,1.,1.]).all(axis=2)\n",
        "  target_grid[mask_target] = [1, 0, 0]\n",
        "\n",
        "\n",
        "  mask_pred = (prediction_grid == [1.,1.,1.]).all(axis=2)\n",
        "  prediction_grid[mask_pred] = [0, 0, 1]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(img_grid, cmap=plt.cm.bone)\n",
        "  plt.imshow(target_grid, alpha=0.3)\n",
        "  plt.imshow(prediction_grid, alpha=0.3)\n",
        "  plt.show()\n",
        "\n",
        "def show_images_with_masks(images, labels):\n",
        "  # create grid of images\n",
        "  img_grid = torchvision.utils.make_grid(images.clone().cpu())\n",
        "  target_grid = torchvision.utils.make_grid(labels.clone().cpu())\n",
        "  # print(img_grid.shape)\n",
        "\n",
        "  img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "  target_grid = np.transpose(target_grid.numpy(), (1, 2, 0))\n",
        "\n",
        "  # print(target_grid.shape)\n",
        "  mask_target = (target_grid == [1.,1.,1.]).all(axis=2)\n",
        "  target_grid[mask_target] = [1, 0, 0]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(img_grid, cmap=plt.cm.bone)\n",
        "  plt.imshow(target_grid, alpha=0.3)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR3yIAUm5yuE",
        "colab_type": "code",
        "outputId": "70e9b1e0-f9a7-4feb-fa5d-d9362c081181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "sample_images, sample_labels = dataiter.next()\n",
        "\n",
        "print(sample_images.shape)\n",
        "print(sample_labels.shape)\n",
        "\n",
        "show_images_with_masks(sample_images, sample_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 1, 224, 224])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAABNCAYAAACsXX8MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDzGOdy3NaI9uAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Q6L1fYmt9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir=\"$NETWORK_OPTIONS_OBJ.TENSORBOARD_LOGS_DIR\"\n",
        "# # %reload_ext tensorboard\n",
        "# # import time\n",
        "# # time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyl_3nh0au9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcAflpuw--We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = SIMMSoftDiceLoss()\n",
        "DiceScore = SIMMHardDiceScore()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFqbfplFcymv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "timestr = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "\n",
        "run_prefix = 'fcdn103'\n",
        "run_name = timestr\n",
        "if NETWORK_OPTIONS_OBJ.CONTINUE_TRAIN == True:\n",
        "  run_name = NETWORK_OPTIONS_OBJ.RUN_NAME\n",
        "print(run_name)\n",
        "\n",
        "tb_log_dir = NETWORK_OPTIONS_OBJ.TENSORBOARD_LOGS_DIR + run_prefix + '_' + run_name\n",
        "tb = SummaryWriter(flush_secs=10, log_dir=tb_log_dir)\n",
        "\n",
        "# criterion = SIMMSoftDiceLoss()\n",
        "best_train_loss = 100.0\n",
        "best_valid_loss = 100.0\n",
        "\n",
        "if NETWORK_OPTIONS_OBJ.CONTINUE_TRAIN == False:\n",
        "  tb.add_scalar(\"learning_rate\", NETWORK_OPTIONS_OBJ.LR_RATE)\n",
        "  tb.add_scalar(\"batch_size\", NETWORK_OPTIONS_OBJ.BATCH_SIZE)\n",
        "  # tb.add_scalar(\"feature_scale\", NETWORK_OPTIONS_OBJ.FEATURE_SCALE)\n",
        "  # tb.add_image('siim_sample_images', img_grid)\n",
        "  tb.add_text('run_name', run_name)\n",
        "  tb.add_text('run_desc', 'FCDenseNet with Attention Gates')\n",
        "  # tb.add_hparams(NETWORK_OPTIONS)\n",
        "else:\n",
        "  best_valid_loss = last_best_valid_loss\n",
        "\n",
        "\n",
        "tb.add_graph(model, sample_images.float().to(device))\n",
        "\n",
        "tb.flush()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NETWORK_OPTIONS_OBJ.START_EPOCH, NETWORK_OPTIONS_OBJ.START_EPOCH + NETWORK_OPTIONS_OBJ.NUM_EPOCHS):\n",
        "  time.sleep(0.1)\n",
        "  print('############# Running epoch: %d...\\n' % (epoch))\n",
        "\n",
        "  ## Training Iterations\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  loss_per_epoch = 0.0\n",
        "  dice_score_per_epoch = 0.0\n",
        "  epoch_batch_count = 0\n",
        "\n",
        "  total_iter = total=len(train_loader)\n",
        "  for epoch_iter, (images, labels) in tqdm(enumerate(train_loader, 1), total=total_iter):\n",
        "    # Make a training update\n",
        "    inputs = images.float().to(device)\n",
        "    masks = labels.to(device)\n",
        "    # assert input.size() == target.size()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "    # print(outputs.shape)\n",
        "    loss = criterion(outputs, masks)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # calculate dice score\n",
        "    dScore = DiceScore(outputs, masks)\n",
        "\n",
        "    # update losses for epoch\n",
        "    loss_per_epoch += loss.item()\n",
        "    dice_score_per_epoch += dScore.item()\n",
        "    epoch_batch_count += 1\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if epoch_iter % 20 == 0:\n",
        "      loss_avg = running_loss / 20\n",
        "      #print('[%d, %5d] Running loss: %.3f' % (epoch + 1, epoch_iter + 1, loss_avg))\n",
        "      i_num = (epoch + 1) + ((epoch_iter + 1) / total_iter)\n",
        "      tb.add_scalar('RunningLoss', loss_avg, i_num)\n",
        "      running_loss = 0.0\n",
        "\n",
        "  loss = loss_per_epoch / epoch_batch_count\n",
        "  dice_score_per_epoch = dice_score_per_epoch / epoch_batch_count\n",
        "  tb.add_scalar('Loss', loss, epoch + 1)\n",
        "  tb.add_scalar('DiceScore', dice_score_per_epoch, epoch + 1)\n",
        "  print('*********** [%d] Loss per epoch: %.3f' %(epoch + 1, loss))\n",
        "  print('*********** [%d] Dice Score per epoch: %.3f' %(epoch + 1, dice_score_per_epoch))\n",
        "\n",
        "  if loss <= best_train_loss:\n",
        "    best_train_loss = loss\n",
        "\n",
        "  # Validation Iterations\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  running_loss_valid = 0.0\n",
        "  loss_per_epoch_valid = 0.0\n",
        "  dice_score_per_epoch_valid = 0.0\n",
        "  epoch_batch_count_valid = 0\n",
        "  loss_valid = 100\n",
        "  if NETWORK_OPTIONS_OBJ.USE_VAL_SET:\n",
        "    with torch.no_grad():\n",
        "      total_valid_iter = total=len(valid_loader)\n",
        "      for epoch_iter, (images, labels) in tqdm(enumerate(valid_loader, 1), total=total_valid_iter):\n",
        "        # get batch\n",
        "        inputs = images.float().to(device)\n",
        "        masks = labels.to(device)\n",
        "        # assert input.size() == target.size()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # calculate dice score\n",
        "        dScore = DiceScore(outputs, masks)\n",
        "\n",
        "        # update losses for epoch\n",
        "        loss_per_epoch_valid += loss.item()\n",
        "        dice_score_per_epoch_valid += dScore.item()\n",
        "        epoch_batch_count_valid += 1\n",
        "        running_loss_valid += loss.item()\n",
        "\n",
        "        if epoch_iter % 20 == 0:\n",
        "          loss_avg = running_loss_valid / 20\n",
        "          # print('[%d, %5d] Running loss Validation: %.3f' % (epoch + 1, epoch_iter + 1, loss_avg))\n",
        "          # (10 * x + y) / 10\n",
        "          # i_num = (10 * (epoch + 1) + (epoch_iter + 1)) - 10\n",
        "          i_num = (epoch + 1) + ((epoch_iter + 1) / total_valid_iter)\n",
        "          tb.add_scalar('RunningLossValidation', loss_avg, i_num)\n",
        "          running_loss_valid = 0.0\n",
        "\n",
        "    loss_valid = loss_per_epoch_valid / epoch_batch_count_valid\n",
        "    dice_score_per_epoch_valid = dice_score_per_epoch_valid / epoch_batch_count_valid\n",
        "    tb.add_scalar('LossValidation', loss_valid, epoch + 1)\n",
        "    tb.add_scalar('DiceScoreValidation', dice_score_per_epoch_valid, epoch + 1)\n",
        "    print('*********** [%d] Validation Loss per epoch: %.3f' %(epoch + 1, loss_valid))\n",
        "    print('*********** [%d] Validation DiceScore per epoch: %.3f' %(epoch + 1, dice_score_per_epoch_valid))\n",
        "\n",
        "\n",
        "  is_best = False\n",
        "  if loss_valid < best_valid_loss:\n",
        "    best_valid_loss = loss_valid\n",
        "    is_best = True\n",
        "\n",
        "  create_checkpoint(run_name, model, optimizer, is_best, epoch, NETWORK_OPTIONS, loss, loss_valid, best_valid_loss)\n",
        "\n",
        "  # reset loss per epoch\n",
        "  loss_per_epoch = 0.0\n",
        "  dice_score_per_epoch = 0.0\n",
        "  epoch_batch_count = 0\n",
        "  # reset validation loss per epoch\n",
        "  loss_per_epoch_valid = 0.0\n",
        "  dice_score_per_epoch_valid = 0.0\n",
        "  epoch_batch_count_valid = 0\n",
        "  \n",
        "  tb.flush()\n",
        "\n",
        "  \n",
        "  # Update the model learning rate\n",
        "  # model.update_learning_rate()\n",
        "tb.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mYq5-Of9mvJ",
        "colab_type": "text"
      },
      "source": [
        "# Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDMAoWUJdChc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate loss on test set\n",
        "loss_test = 0.0\n",
        "dScore_test = 0.0\n",
        "batch_count_test = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# loader = DataLoader(dataset=train_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for epoch_iter, (images, labels) in tqdm(enumerate(test_loader, 1), total=len(test_loader)):\n",
        "    # get batch\n",
        "    inputs = images.float().to(device)\n",
        "    masks = labels.to(device)\n",
        "    # assert input.size() == target.size()\n",
        "\n",
        "    # forward\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, masks)\n",
        "    dScore = DiceScore(outputs.clone(), masks.clone())\n",
        "\n",
        "    # # update losses for epoch\n",
        "    loss_test += loss.item()\n",
        "    dScore_test += dScore.item()\n",
        "    batch_count_test += 1\n",
        "\n",
        "    # print('\\n -- Loss on Test Set: %.3f' %(loss.item()))\n",
        "    # print('\\n -- Dice score on Test Set: %.3f' %(dScore.item()))\n",
        "\n",
        "loss_test = loss_test / batch_count_test\n",
        "dScore_test = dScore_test / batch_count_test\n",
        "# tb.add_scalar('LossTest', loss_test, loss_test)\n",
        "print('\\n Loss on Test Set: %.3f' %(loss_test))\n",
        "print('\\n Dice score on Test Set: %.3f' %(dScore_test))\n",
        "# tb.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiCq1ACI9_E4",
        "colab_type": "text"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMVpaVfTSxzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_heatmap(images):\n",
        "  # create grid of images\n",
        "  img_grid = torchvision.utils.make_grid(images.clone().cpu())\n",
        "  img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(img_grid)\n",
        "  # plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1xP6qhRQr3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def save_images_with_masks_and_prediction(images, labels, predictions, file_name):\n",
        "#   # create grid of images\n",
        "#   img_grid = torchvision.utils.make_grid(images.clone().cpu())\n",
        "#   target_grid = torchvision.utils.make_grid(labels.clone().cpu())\n",
        "#   prediction_grid = torchvision.utils.make_grid(predictions.clone().cpu())\n",
        "#   # print(img_grid.shape)\n",
        "\n",
        "#   img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "#   target_grid = np.transpose(target_grid.numpy(), (1, 2, 0))\n",
        "#   prediction_grid = np.transpose(prediction_grid.numpy(), (1, 2, 0))\n",
        "\n",
        "#   mask_target = (target_grid == [1.,1.,1.]).all(axis=2)\n",
        "#   target_grid[mask_target] = [1, 0, 0]\n",
        "\n",
        "#   mask_pred = (prediction_grid == [1.,1.,1.]).all(axis=2)\n",
        "#   prediction_grid[mask_pred] = [0, 0, 1]\n",
        "\n",
        "#   plt.figure()\n",
        "#   plt.imshow(img_grid, cmap=plt.cm.bone)\n",
        "#   plt.imshow(target_grid, alpha=0.3)\n",
        "#   plt.imshow(prediction_grid, alpha=0.3)\n",
        "#   plt.savefig(file_name)\n",
        "#   plt.show()\n",
        "\n",
        "# def save_att_cof(images, file_name):\n",
        "#   # create grid of images\n",
        "#   img_grid = torchvision.utils.make_grid(images.clone().cpu())\n",
        "#   img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "\n",
        "#   plt.figure()\n",
        "#   plt.imshow(img_grid)\n",
        "#   plt.savefig(file_name)\n",
        "#   plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klyik6VdQ2Ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# m_name = NETWORK_OPTIONS_OBJ.RUN_NAME\n",
        "# RESULT_IMG_DIR = IMG_RESULT_SAVE_DIR + m_name\n",
        "# if os.path.exists(RESULT_IMG_DIR):\n",
        "#   shutil.rmtree(RESULT_IMG_DIR)\n",
        "#   print(\"Deleted RESULT_IMG_DIR directory: \" + RESULT_IMG_DIR)\n",
        "\n",
        "# os.makedirs(RESULT_IMG_DIR)\n",
        "# print(\"Created directory: \" + RESULT_IMG_DIR)\n",
        "# %ls '$RESULT_IMG_DIR'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc3iJReEQour",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from IPython.display import clear_output\n",
        "\n",
        "# model.eval()\n",
        "\n",
        "# loader = DataLoader(dataset=test_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=2, shuffle=False)\n",
        "\n",
        "# print(\"Red is Ground Truth.\")\n",
        "# print(\"Blue is prediction.\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   for epoch_iter, (images, labels) in tqdm(enumerate(loader, 1), total=len(loader)):\n",
        "#     print(\"Iter: \" + str(epoch_iter))\n",
        "#     inputs = images.float().to(device)\n",
        "#     masks = labels.to(device)\n",
        "#     outputs = model(inputs)\n",
        "#     att_coffs = model.getLastAttentionCoefficients()\n",
        "\n",
        "#     outputs[outputs > 0.5] = 1\n",
        "#     outputs[outputs <= 0.5] = 0\n",
        "#     fileName = RESULT_IMG_DIR + '/' + str(epoch_iter) + '_test_pred' + '.png'\n",
        "#     save_images_with_masks_and_prediction(images, labels, outputs, fileName)\n",
        "\n",
        "#     fileName3 = RESULT_IMG_DIR + '/' + str(epoch_iter) + '_test_att_3' + '.png'\n",
        "#     save_att_cof(att_coffs[3], fileName3)\n",
        "\n",
        "#     fileName2 = RESULT_IMG_DIR + '/' + str(epoch_iter) + '_test_att_2' + '.png'\n",
        "#     save_att_cof(att_coffs[2], fileName2)\n",
        "\n",
        "#     fileName1 = RESULT_IMG_DIR + '/' + str(epoch_iter) + '_test_att_1' + '.png'\n",
        "#     save_att_cof(att_coffs[1], fileName1)\n",
        "\n",
        "#     fileName0 = RESULT_IMG_DIR + '/' + str(epoch_iter) + '_test_att_0' + '.png'\n",
        "#     save_att_cof(att_coffs[0], fileName0)\n",
        "#     clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwidP8oe-Cu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# loader = train_loader\n",
        "# # loader = valid_loader\n",
        "loader = DataLoader(dataset=test_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=2, shuffle=False)\n",
        "\n",
        "max_display = 2\n",
        "display_count = 0\n",
        "\n",
        "print(\"Red is Ground Truth.\")\n",
        "print(\"Blue is prediction.\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  for epoch_iter, (images, labels) in tqdm(enumerate(loader, 1), total=len(loader)):\n",
        "    print(\"Iter: \" + str(epoch_iter))\n",
        "    # get batch\n",
        "    inputs = images.float().to(device)\n",
        "    masks = labels.to(device)\n",
        "\n",
        "    # forward\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    att_coffs = model.getLastAttentionCoefficients()\n",
        "\n",
        "    outputs[outputs > 0.5] = 1\n",
        "    outputs[outputs <= 0.5] = 0\n",
        "\n",
        "    show_images_with_masks_and_prediction(images, labels, outputs)\n",
        "    show_heatmap(att_coffs[3])\n",
        "\n",
        "    display_count += 1\n",
        "    if display_count >= max_display:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}