{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIIM_all.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeZ0dtNrgZ9u",
        "colab_type": "text"
      },
      "source": [
        "## Install dependencies and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7gsu-kmmD3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u92getfvZrFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install scipy\n",
        "!pip install torchvision\n",
        "!pip install tqdm\n",
        "!pip install visdom\n",
        "!pip install nibabel\n",
        "!pip install scikit-image\n",
        "!pip install h5py\n",
        "!pip install pandas\n",
        "!pip install dominate\n",
        "!pip install pydicom\n",
        "!pip install opencv-python\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install https://github.com/ozan-oktay/torchsample/tarball/master#egg=torchsample-0.1.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXBjENLveAnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import json\n",
        "import collections\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import numbers\n",
        "import PIL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_3QqLS-V6yC",
        "colab_type": "text"
      },
      "source": [
        "## Setup dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGduGTl4WqGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# local machine directories\n",
        "DATA_DIR = \"/content/data/\"\n",
        "DATA_MASKS_DIR = DATA_DIR + \"masks\"\n",
        "DATA_DICOM_DIR = DATA_DIR + \"input\"\n",
        "\n",
        "# google drive directories\n",
        "DATA_ZIP_FILE_PATH = \"/content/gdrive/My Drive/mlmi/dataset/input.zip\"\n",
        "MASKS_DIR = \"/content/gdrive/My Drive/mlmi/dataset/masks/\"\n",
        "TENSORBOARD_LOGS_DIR = \"/content/gdrive/My Drive/mlmi/results/tensorboard/\"\n",
        "MODEL_SAVE_DIR = \"/content/gdrive/My Drive/mlmi/results/model/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XITnSUqxZkXe",
        "colab_type": "text"
      },
      "source": [
        "Copy masks from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg9GPhgkcCF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create data directory locally\n",
        "if os.path.exists(DATA_DIR) == False:\n",
        "  os.makedirs(DATA_DIR)\n",
        "  print(\"Create directory: \" + DATA_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yof8muQ0sdOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(DATA_MASKS_DIR):\n",
        "  shutil.rmtree(DATA_MASKS_DIR)\n",
        "  print(\"Deleted old masks directory: \" + DATA_MASKS_DIR)\n",
        "\n",
        "!cp -r \"$MASKS_DIR\" $DATA_DIR\n",
        "%ls $DATA_MASKS_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_wBRjKYXuFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### uncomment this code to reduce the train set size\n",
        "# includeImagesCounts = 50\n",
        "# trainSetCsv = pd.read_csv(DATA_MASKS_DIR + '/simm_DS_train.csv')\n",
        "# tsReduced = trainSetCsv[:includeImagesCounts]\n",
        "# tsReduced[:5]\n",
        "\n",
        "# tsReduced.to_csv(DATA_MASKS_DIR + '/simm_DS_train.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBzkrPwNfdZ_",
        "colab_type": "text"
      },
      "source": [
        "Extract DICOM files from zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg8ThUO3mJhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only extract if input folder doesn't exists\n",
        "if os.path.exists(DATA_DICOM_DIR) == False:\n",
        "  #!unzip \"$DATA_ZIP_FILE_PATH\" -d \"$DATA_DIR\"\n",
        "  zip_ref = zipfile.ZipFile(DATA_ZIP_FILE_PATH, 'r')\n",
        "  zip_ref.extractall(DATA_DIR)\n",
        "  zip_ref.close()\n",
        "  print(\"Extracted dicom zip file to directory: \" + DATA_DICOM_DIR)\n",
        "\n",
        "%ls $DATA_DICOM_DIR\n",
        "#%ls \"$DATA_DICOM_DIR/siim/dicom-images-train\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6dCJ9tQhEyJ",
        "colab_type": "text"
      },
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdZCyDaITnn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(net, init_type='normal'):\n",
        "    #print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    elif init_type == 'orthogonal':\n",
        "        net.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant(m.bias.data, 0.0)\n",
        "        \n",
        "class unetConv2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
        "        super(unetConv2, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = ks\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        s = stride\n",
        "        p = padding\n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n+1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.BatchNorm2d(out_size),\n",
        "                                     nn.ReLU(inplace=True),)\n",
        "                setattr(self, 'conv%d'%i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n+1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.ReLU(inplace=True),)\n",
        "                setattr(self, 'conv%d'%i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n+1):\n",
        "            conv = getattr(self, 'conv%d'%i)\n",
        "            x = conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class unetUp2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, is_batchnorm=True):\n",
        "        super(unetUp2, self).__init__()\n",
        "        self.conv = unetConv2(in_size + out_size, out_size, is_batchnorm)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('unetConv2') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        outputs2 = self.up(inputs2)\n",
        "        # offset = outputs2.size()[1] - inputs1.size()[1]\n",
        "        # padding = [0, 0, 0, 0, offset // 2, offset // 2]\n",
        "        # outputs1 = F.pad(inputs1, padding)\n",
        "        outputs1 = inputs1\n",
        "        cat = torch.cat([outputs1, outputs2], 1)\n",
        "        return self.conv(cat)\n",
        "\n",
        "class UnetGridGatingSignal2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, ks=1, is_batchnorm=True):\n",
        "        super(UnetGridGatingSignal2, self).__init__()\n",
        "\n",
        "        if is_batchnorm:\n",
        "            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, ks),\n",
        "                                       nn.BatchNorm2d(out_size),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, ks),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       )\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        return outputs\n",
        "\n",
        "class UnetDsv2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, scale_factor):\n",
        "        super(UnetDsv2, self).__init__()\n",
        "        self.dsv = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0),\n",
        "                                 nn.Upsample(scale_factor=scale_factor, mode='bilinear'), )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.dsv(input)\n",
        "\n",
        "class _GridAttentionBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3, mode='concatenation',\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(_GridAttentionBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [2, 3]\n",
        "        assert mode in ['concatenation', 'concatenation_debug', 'concatenation_residual']\n",
        "\n",
        "        # Downsampling rate for the input featuremap\n",
        "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
        "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
        "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
        "\n",
        "        # Default parameter set\n",
        "        self.mode = mode\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
        "\n",
        "        # Number of channels (pixel dimensions)\n",
        "        self.in_channels = in_channels\n",
        "        self.gating_channels = gating_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            bn = nn.BatchNorm3d\n",
        "            self.upsample_mode = 'trilinear'\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            bn = nn.BatchNorm2d\n",
        "            self.upsample_mode = 'bilinear'\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # Output transform\n",
        "        self.W = nn.Sequential(\n",
        "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
        "            bn(self.in_channels),\n",
        "        )\n",
        "\n",
        "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        # Initialise weights\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "        # Define the operation\n",
        "        if mode == 'concatenation':\n",
        "            self.operation_function = self._concatenation\n",
        "        elif mode == 'concatenation_debug':\n",
        "            self.operation_function = self._concatenation_debug\n",
        "        elif mode == 'concatenation_residual':\n",
        "            self.operation_function = self._concatenation_residual\n",
        "        else:\n",
        "            raise NotImplementedError('Unknown operation function.')\n",
        "\n",
        "\n",
        "    def forward(self, x, g):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :param g: (b, g_d)\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        output = self.operation_function(x, g)\n",
        "        return output\n",
        "\n",
        "    def _concatenation(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.interpolate(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.relu(theta_x + phi_g, inplace=True)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        sigm_psi_f = torch.sigmoid(self.psi(f))\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.interpolate(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(GridAttentionBlock2D, self).__init__(in_channels,\n",
        "                                                   inter_channels=inter_channels,\n",
        "                                                   gating_channels=gating_channels,\n",
        "                                                   dimension=2, mode=mode,\n",
        "                                                   sub_sample_factor=sub_sample_factor\n",
        "                                                   )\n",
        "\n",
        "\n",
        "class MultiAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_size, gate_size, inter_size, nonlocal_mode, sub_sample_factor):\n",
        "        super(MultiAttentionBlock, self).__init__()\n",
        "        self.gate_block_1 = GridAttentionBlock2D(in_channels=in_size, gating_channels=gate_size,\n",
        "                                                 inter_channels=inter_size, mode=nonlocal_mode,\n",
        "                                                 sub_sample_factor= sub_sample_factor)\n",
        "        self.combine_gates = nn.Sequential(nn.Conv2d(in_size, in_size, kernel_size=1, stride=1, padding=0),\n",
        "                                           nn.BatchNorm2d(in_size),\n",
        "                                           nn.ReLU(inplace=True)\n",
        "                                           )\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('GridAttentionBlock2D') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, input, gating_signal):\n",
        "        gate_1, attention_1 = self.gate_block_1(input, gating_signal)\n",
        "        return self.combine_gates(gate_1), attention_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD7Tm0ZQTdac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class unet_simm(nn.Module):\n",
        "\n",
        "    def __init__(self, feature_scale, n_classes, is_deconv, in_channels,\n",
        "                 nonlocal_mode, attention_dsample, is_batchnorm):\n",
        "        super(unet_simm, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.feature_scale = feature_scale\n",
        "        # self.use_cuda = Config.use_cuda\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "        filters = [int(x / self.feature_scale) for x in filters]\n",
        "        # filter [16, 32, 64, 128, 256]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv1 = unetConv2(self.in_channels, filters[0], self.is_batchnorm, ks=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm, ks=3)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm, ks=3)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm, ks=3)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.center = unetConv2(filters[3], filters[4], self.is_batchnorm, ks=3)\n",
        "        self.gating = UnetGridGatingSignal2(filters[4], filters[4], ks=1, is_batchnorm=self.is_batchnorm)\n",
        "\n",
        "        # attention blocks\n",
        "        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],\n",
        "                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n",
        "        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],\n",
        "                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n",
        "        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],\n",
        "                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat4 = unetUp2(filters[4], filters[3], False, is_batchnorm)\n",
        "        # self.up_concat4 = unetUp2(512, filters[3], False, is_batchnorm)\n",
        "        self.up_concat3 = unetUp2(filters[3], filters[2], False, is_batchnorm)\n",
        "        self.up_concat2 = unetUp2(filters[2], filters[1], False, is_batchnorm)\n",
        "        self.up_concat1 = unetUp2(filters[1], filters[0], False, is_batchnorm)\n",
        "\n",
        "        # deep supervision\n",
        "        self.dsv4 = UnetDsv2(in_size=filters[3], out_size=n_classes, scale_factor=8)\n",
        "        self.dsv3 = UnetDsv2(in_size=filters[2], out_size=n_classes, scale_factor=4)\n",
        "        self.dsv2 = UnetDsv2(in_size=filters[1], out_size=n_classes, scale_factor=2)\n",
        "        self.dsv1 = nn.Conv2d(in_channels=filters[0], out_channels=n_classes, kernel_size=1)\n",
        "\n",
        "        # final conv (without any concat)\n",
        "        self.final = nn.Conv2d(n_classes*4, n_classes, 1)\n",
        "\n",
        "        # initialise weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # if self.use_cuda == False:\n",
        "        #     # inputs = inputs.to(dtype=torch.float64)\n",
        "        #     inputs = inputs.double()\n",
        "\n",
        "        # inputs = inputs.double()\n",
        "\n",
        "        # Feature Extraction\n",
        "        conv1 = self.conv1(inputs)\n",
        "        maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "        conv2 = self.conv2(maxpool1)\n",
        "        maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "        conv3 = self.conv3(maxpool2)\n",
        "        maxpool3 = self.maxpool3(conv3)\n",
        "\n",
        "        conv4 = self.conv4(maxpool3)\n",
        "        maxpool4 = self.maxpool4(conv4)\n",
        "\n",
        "        # Gating Signal Generation\n",
        "        center = self.center(maxpool4)\n",
        "        gating = self.gating(center)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        # Upscaling Part (Decoder)\n",
        "        g_conv4, att4 = self.attentionblock4(conv4, gating)\n",
        "        up4 = self.up_concat4(g_conv4, center)\n",
        "        g_conv3, att3 = self.attentionblock3(conv3, up4)\n",
        "        up3 = self.up_concat3(g_conv3, up4)\n",
        "        g_conv2, att2 = self.attentionblock2(conv2, up3)\n",
        "        up2 = self.up_concat2(g_conv2, up3)\n",
        "        up1 = self.up_concat1(conv1, up2)\n",
        "\n",
        "        # Deep Supervision\n",
        "        dsv4 = self.dsv4(up4)\n",
        "        dsv3 = self.dsv3(up3)\n",
        "        dsv2 = self.dsv2(up2)\n",
        "        dsv1 = self.dsv1(up1)\n",
        "        final = self.final(torch.cat([dsv1,dsv2,dsv3,dsv4], dim=1))\n",
        "\n",
        "        final = torch.sigmoid(final)\n",
        "\n",
        "        return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq628aADYjqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width* height)\n",
        "    array = np.asarray([int(x) for x in rle.split()])\n",
        "    starts = array[0::2]\n",
        "    lengths = array[1::2]\n",
        "\n",
        "    current_position = 0\n",
        "    for index, start in enumerate(starts):\n",
        "        current_position += start\n",
        "        mask[current_position:current_position+lengths[index]] = 255\n",
        "        current_position += lengths[index]\n",
        "\n",
        "    return mask.reshape(width, height)\n",
        "    \n",
        "class SIMMDataset(Dataset):\n",
        "    \"\"\"SIMM dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, split, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dicomPaths (Array<string>): Array of DICOM file Paths.\n",
        "            mask_csv_file (string): csv file with encoded masks (rle).\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.dir_postfix = 'input/siim/'\n",
        "\n",
        "        self.im_height = 1024\n",
        "        self.im_width = 1024\n",
        "        self.im_chan = 1\n",
        "\n",
        "        ## Read masks file\n",
        "        mask_csv_file = root_dir + 'masks/train-rle.csv' \n",
        "        print(\"Reading masks from: \" + mask_csv_file)\n",
        "        self.encodedMasks = pd.read_csv(mask_csv_file, names=['ImageId', 'EncodedPixels'], index_col='ImageId')\n",
        "\n",
        "        ## Read dataset file names\n",
        "        dsFile = root_dir + 'masks/simm_DS_' + split + '.csv'\n",
        "        print(\"Reading ds from: \" + dsFile)\n",
        "        dsFileData = pd.read_csv(dsFile)\n",
        "        self.dicomPaths = dsFileData['path'].tolist()\n",
        "        # print(\"READ\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dicomPaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Read dicom file\n",
        "        dPath = self.dicomPaths[idx]\n",
        "        dicom = pydicom.dcmread(self.root_dir + self.dir_postfix + dPath)\n",
        "        \n",
        "        # get image from dicom to numpy array\n",
        "        image = np.array(dicom.pixel_array)\n",
        "        \n",
        "        # get mask (in rle) from csv\n",
        "        landmarks = np.zeros((self.im_height, self.im_width), dtype=np.bool)\n",
        "        \n",
        "        fileId = dPath.split('/')[-1][:-4]\n",
        "        rle = self.encodedMasks.loc[fileId, 'EncodedPixels']\n",
        "        try:\n",
        "            if type(rle) == str: # if single rle\n",
        "                decodedRle = rle2mask(rle, self.im_height, self.im_width)\n",
        "#                 landmarks = np.expand_dims(decodedRle, axis=2)\n",
        "                landmarks = decodedRle\n",
        "            else: # if multiple rle\n",
        "                for x in rle:\n",
        "                    decodedRle = rle2mask(x, self.im_height, self.im_width)\n",
        "                    landmarks = landmarks + decodedRle\n",
        "#                     landmarks = landmarks + np.expand_dims(decodedRle, axis=2)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            \n",
        "        #### TODO - IMPORTANT::: CHECK THIS  \n",
        "        ## QUESTION: SHOULD WE TRANSPOSE THE MASK IN THE GETITEM FUNCTION \n",
        "        ## BECAUSE WHEN PLOTING THE GRAPHS WE HAVE TO TRANSPOSE IT.\n",
        "        landmarks = landmarks.T\n",
        "\n",
        "        # for some images, we have multiple masks, so we are adding the masks\n",
        "        # which results in some pixels to > 1\n",
        "        landmarks = (landmarks >= 1).astype('float64')\n",
        "\n",
        "        if self.transform:\n",
        "          # converting numpy array to PIL image\n",
        "          # because torchvision transforms needs PIL image\n",
        "          image = PIL.Image.fromarray(image)\n",
        "          landmarks = PIL.Image.fromarray(landmarks)\n",
        "\n",
        "          # apply transformation\n",
        "          image, landmarks = self.transform(image, landmarks)\n",
        "\n",
        "          # converting back to numpy array from PIL image\n",
        "          image = np.array(image)\n",
        "          landmarks = np.array(landmarks)\n",
        "\n",
        "        img = np.expand_dims(image, axis=0)\n",
        "        mk = np.expand_dims(landmarks, axis=0)\n",
        "\n",
        "        # print(\"Final IMG:\")\n",
        "        # print(type(img))\n",
        "        # print(img.shape)\n",
        "\n",
        "        # normalize to 0-1\n",
        "        img = img / 255\n",
        "\n",
        "        return img, mk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOuDOXyMaAtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SIMMSoftDiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIMMSoftDiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        smooth = 0.01\n",
        "        batch_size = input.size(0)\n",
        "\n",
        "        # input = torch.sigmoid(input).view(batch_size, -1)\n",
        "        input = input.view(batch_size, -1)\n",
        "        target = target.contiguous().view(batch_size, -1)\n",
        "\n",
        "        inter = torch.sum(input * target, 1) + smooth\n",
        "        union = torch.sum(input, 1) + torch.sum(target, 1) + smooth\n",
        "\n",
        "        score = torch.sum(2.0 * inter / union, 0)\n",
        "        score = 1.0 - score / float(batch_size)\n",
        "        \n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3trCAF_ekaxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def json_file_to_pyobj(jsonStr):\n",
        "    def _json_object_hook(d): return collections.namedtuple('X', d.keys())(*d.values())\n",
        "    def json2obj(data): return json.loads(data, object_hook=_json_object_hook)\n",
        "    return json2obj(jsonStr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyWrBfFyLM-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_checkpoint(run_name, model, optimizer, is_best, epoch, network_options, train_loss, valid_loss, best_valid_loss):\n",
        "\n",
        "  save_dir = os.path.join(MODEL_SAVE_DIR, run_name)\n",
        "  if os.path.exists(save_dir) == False:\n",
        "    os.makedirs(save_dir)\n",
        "    print('Creating save dir: ' + save_dir)\n",
        "\n",
        "  print('Saving model checkpoint at epoch: ' + str(epoch))\n",
        "  state = {\n",
        "      'run_name': run_name,\n",
        "      'model_state': model.state_dict(),\n",
        "      'optimizer_state': optimizer.state_dict(),\n",
        "      'epoch': epoch,\n",
        "      'rng_state': torch.get_rng_state(),\n",
        "      'network_options': network_options,\n",
        "      'train_loss': train_loss,\n",
        "      'valid_loss': valid_loss,\n",
        "      'best_valid_loss': best_valid_loss\n",
        "  }\n",
        "\n",
        "  torch.save(state, os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint.pt'))\n",
        "  if is_best:\n",
        "    torch.save(state, os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint_best.pt'))\n",
        "\n",
        "def load_checkpoint(run_name, model, optimizer):\n",
        "  ckh_path = os.path.join(MODEL_SAVE_DIR, run_name, 'checkpoint.pt')\n",
        "  checkpoint = torch.load(ckh_path)\n",
        "  model.load_state_dict(checkpoint['model_state'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "  torch.set_rng_state(checkpoint['rng_state'])\n",
        "\n",
        "  return model, optimizer, checkpoint\n",
        "\n",
        "def print_checkpoint(checkpoint):\n",
        "  print(\"Run Name: \" + str(checkpoint['run_name']))\n",
        "  print(\"Last epoch: \" + str(checkpoint['epoch']))\n",
        "  print(\"Train Loss: \" + str(checkpoint['train_loss']))\n",
        "  print(\"Validation Loss: \" + str(checkpoint['valid_loss']))\n",
        "  print(\"Best Validation Loss: \" + str(checkpoint['best_valid_loss']))\n",
        "  print(\"Options: \" + str(checkpoint['network_options']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD7YUvq0tLnR",
        "colab_type": "text"
      },
      "source": [
        "Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1RkAFHbtKWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SegCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class SegRandomAffine(object):\n",
        "  def __init__(self, degrees, translate=None, scale=None, resample=False, fillcolor=0):\n",
        "    # NOTE: shear is REMOVED FOR NOW\n",
        "\n",
        "    # just to check the values using this contructor\n",
        "    self.segTransform = T.RandomAffine(degrees, translate, scale, None, resample, fillcolor)\n",
        "\n",
        "    if isinstance(degrees, numbers.Number):\n",
        "        self.degrees = (-degrees, degrees)\n",
        "    else:\n",
        "        self.degrees = degrees\n",
        "        \n",
        "    self.translate = translate\n",
        "    self.scale = scale\n",
        "    self.resample = resample\n",
        "    self.fillcolor = fillcolor\n",
        "\n",
        "  def __call__(self, image, target):\n",
        "        ret = T.RandomAffine.get_params(self.degrees, self.translate, self.scale, None, image.size)\n",
        "        image = T.functional.affine(image, *ret, resample=self.resample, fillcolor=self.fillcolor)\n",
        "        target = T.functional.affine(target, *ret, resample=self.resample, fillcolor=self.fillcolor)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "class SegToTensor(object):\n",
        "  def __init__(self):\n",
        "    self.ToTensor = T.ToTensor()\n",
        "  def __call__(self, image, target):\n",
        "        image = self.ToTensor(image)\n",
        "        target = self.ToTensor(target)\n",
        "\n",
        "        return image, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6gvdAuEhtzH",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuFU4n1AXaG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = False\n",
        "if torch.cuda.is_available():\n",
        "  gpu_count = torch.cuda.device_count()\n",
        "  print(\"Available GPU count:\" + str(gpu_count))\n",
        "  use_cuda = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-M4zJvJh1v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model Hyperparameters\n",
        "\n",
        "NETWORK_OPTIONS = dict()\n",
        "NETWORK_OPTIONS['USE_CUDA'] = use_cuda\n",
        "\n",
        "# Set to false if only need to run on train dataset\n",
        "NETWORK_OPTIONS['USE_VAL_SET'] = True\n",
        "\n",
        "# NETWORK_OPTIONS['LR_POLICY'] = \"step\"\n",
        "# NETWORK_OPTIONS['LR_DECAY_ITERS'] = 25\n",
        "NETWORK_OPTIONS['LR_RATE'] = 1e-6\n",
        "NETWORK_OPTIONS['L2_REG_WEIGHT'] = 1e-6\n",
        "\n",
        "NETWORK_OPTIONS['BATCH_SIZE'] = 2\n",
        "\n",
        "# dont forget to set run_name when continue_train is true\n",
        "NETWORK_OPTIONS['CONTINUE_TRAIN'] = False\n",
        "NETWORK_OPTIONS['RUN_NAME'] = ''\n",
        "NETWORK_OPTIONS['START_EPOCH'] = 0\n",
        "NETWORK_OPTIONS['NUM_EPOCHS'] = 100\n",
        "\n",
        "NETWORK_OPTIONS['FEATURE_SCALE'] = 2\n",
        "NETWORK_OPTIONS['DIVISION_FACTOR'] = 1\n",
        "\n",
        "NETWORK_OPTIONS['DATALOADER_NUM_WORKERS'] = 1\n",
        "NETWORK_OPTIONS['DATA_DIR'] = DATA_DIR\n",
        "NETWORK_OPTIONS['TENSORBOARD_LOGS_DIR'] = TENSORBOARD_LOGS_DIR\n",
        "NETWORK_OPTIONS['MODEL_SAVE_DIR'] = MODEL_SAVE_DIR\n",
        "\n",
        "# converting dict() to python object (just for easy access)\n",
        "NETWORK_OPTIONS_OBJ = json_file_to_pyobj(json.dumps(NETWORK_OPTIONS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SNziec4bb8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining Dataloaders\n",
        "\n",
        "rotate_val = 20.0\n",
        "translation_val = [0.1,0.1]\n",
        "scale_val = [0.7,1.3]\n",
        "\n",
        "train_transform = SegCompose([\n",
        "                              SegRandomAffine(degrees=rotate_val, translate=translation_val, scale=scale_val)\n",
        "                             ])\n",
        "\n",
        "train_dataset = SIMMDataset(NETWORK_OPTIONS_OBJ.DATA_DIR, split='train', transform=train_transform)\n",
        "valid_dataset = SIMMDataset(NETWORK_OPTIONS_OBJ.DATA_DIR, split='validation', transform=None)\n",
        "test_dataset  = SIMMDataset(NETWORK_OPTIONS_OBJ.DATA_DIR, split='test', transform=None)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  num_workers=NETWORK_OPTIONS_OBJ.DATALOADER_NUM_WORKERS, batch_size=NETWORK_OPTIONS_OBJ.BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEsBmffec2SS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup the NN Model\n",
        "model = unet_simm(n_classes=1,\n",
        "                      is_batchnorm=True,\n",
        "                      in_channels=1,\n",
        "                      nonlocal_mode='concatenation',\n",
        "                      feature_scale=NETWORK_OPTIONS_OBJ.FEATURE_SCALE,\n",
        "                      attention_dsample=(2,2),\n",
        "                      is_deconv=False)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=NETWORK_OPTIONS_OBJ.LR_RATE,\n",
        "                        betas=(0.9, 0.999),\n",
        "                        weight_decay=NETWORK_OPTIONS_OBJ.L2_REG_WEIGHT)\n",
        "\n",
        "# optimizer = optim.SGD(params,\n",
        "#                               lr=option.lr_rate,\n",
        "#                               momentum=0.9,\n",
        "#                               nesterov=True,\n",
        "#                               weight_decay=option.l2_reg_weight)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lakj338zM-r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_best_valid_loss = 100.0\n",
        "if NETWORK_OPTIONS_OBJ.CONTINUE_TRAIN == True:\n",
        "  run_name = NETWORK_OPTIONS_OBJ.RUN_NAME\n",
        "  model, optimizer, checkpoint = load_checkpoint(run_name, model, optimizer)\n",
        "  last_best_valid_loss = checkpoint['best_valid_loss']\n",
        "  print_checkpoint(checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR3yIAUm5yuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_images_grid(images):\n",
        "  img_grid = torchvision.utils.make_grid(images, nrow=4)\n",
        "  img_grid = img_grid.numpy()\n",
        "  return np.transpose(img_grid, (1, 2, 0))\n",
        "\n",
        "def show_image(images, labels):\n",
        "  imgs = images.clone()\n",
        "\n",
        "  imgs[labels == 1] = 255\n",
        "  imgs[predictions >= 0.5] = 200\n",
        "\n",
        "  imgs_grid = get_images_grid(imgs) \n",
        "\n",
        "  plt.imshow(imgs_grid, cmap=plt.cm.bone)\n",
        "  plt.show()\n",
        "  del imgs\n",
        "\n",
        "def show_images_with_masks(images, labels):\n",
        "  imgs_grid = get_images_grid(images)\n",
        "  lbls_grid = get_images_grid(labels)\n",
        "\n",
        "  plt.imshow(imgs_grid, cmap=plt.cm.bone)\n",
        "  plt.imshow(lbls_grid, alpha=0.3, cmap=\"Reds\")\n",
        "  plt.show()\n",
        "\n",
        "def show_images_with_masks_and_prediction(images, labels, predictions):\n",
        "  imgs = images.clone()\n",
        "\n",
        "  imgs[labels == 1] = 255\n",
        "  imgs[predictions >= 0.5] = 200\n",
        "\n",
        "  imgs_grid = get_images_grid(imgs) \n",
        "\n",
        "  plt.imshow(imgs_grid, cmap=plt.cm.bone)\n",
        "  plt.show()\n",
        "  del imgs\n",
        "\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "  # if one_channel:\n",
        "  #     img = img.mean(dim=0)\n",
        "  # img = img / 2 + 0.5     # unnormalize\n",
        "  npimg = img.numpy()\n",
        "  if one_channel:\n",
        "      plt.imshow(npimg, cmap=plt.cm.bone)\n",
        "  else:\n",
        "      plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "sample_images, sample_labels = dataiter.next()\n",
        "\n",
        "ims = sample_images.clone()\n",
        "ims[sample_labels == 1] = 255\n",
        "\n",
        "# create grid of images\n",
        "print(ims.shape)\n",
        "img_grid = torchvision.utils.make_grid(ims)\n",
        "print(img_grid.shape)\n",
        "\n",
        "matplotlib_imshow(img_grid, one_channel=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Q6L1fYmt9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"$NETWORK_OPTIONS_OBJ.TENSORBOARD_LOGS_DIR\"\n",
        "# %reload_ext tensorboard\n",
        "import time\n",
        "time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyl_3nh0au9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFqbfplFcymv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "timestr = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "\n",
        "run_name = timestr\n",
        "if NETWORK_OPTIONS_OBJ.CONTINUE_TRAIN == True:\n",
        "  run_name = NETWORK_OPTIONS_OBJ.RUN_NAME\n",
        "print(run_name)\n",
        "\n",
        "tb_log_dir = NETWORK_OPTIONS_OBJ.TENSORBOARD_LOGS_DIR + 'run_' + run_name\n",
        "tb = SummaryWriter(flush_secs=10, log_dir=tb_log_dir)\n",
        "\n",
        "criterion = SIMMSoftDiceLoss()\n",
        "best_train_loss = 100.0\n",
        "best_valid_loss = 100.0\n",
        "\n",
        "if NETWORK_OPTIONS_OBJ.CONTINUE_TRAIN == False:\n",
        "  tb.add_scalar(\"learning_rate\", NETWORK_OPTIONS_OBJ.LR_RATE)\n",
        "  tb.add_scalar(\"batch_size\", NETWORK_OPTIONS_OBJ.BATCH_SIZE)\n",
        "  tb.add_scalar(\"feature_scale\", NETWORK_OPTIONS_OBJ.FEATURE_SCALE)\n",
        "  tb.add_image('siim_sample_images', img_grid)\n",
        "  tb.add_text('run_name', run_name)\n",
        "  # tb.add_hparams(NETWORK_OPTIONS)\n",
        "else:\n",
        "  best_valid_loss = last_best_valid_loss\n",
        "\n",
        "\n",
        "# tb.add_graph(model, sample_images)\n",
        "\n",
        "tb.flush()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NETWORK_OPTIONS_OBJ.START_EPOCH, NETWORK_OPTIONS_OBJ.START_EPOCH + NETWORK_OPTIONS_OBJ.NUM_EPOCHS):\n",
        "  print('############# Running epoch: %d...\\n' % (epoch))\n",
        "\n",
        "  # Training Iterations\n",
        "  running_loss = 0.0\n",
        "  loss_per_epoch = 0.0\n",
        "  epoch_batch_count = 0\n",
        "\n",
        "  total_iter = total=len(train_loader)\n",
        "  for epoch_iter, (images, labels) in tqdm(enumerate(train_loader, 1), total=total_iter):\n",
        "    # Make a training update\n",
        "    inputs = images.float().to(device)\n",
        "    masks = labels.to(device)\n",
        "    # assert input.size() == target.size()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, masks)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # update losses for epoch\n",
        "    loss_per_epoch += loss.item()\n",
        "    epoch_batch_count += 1\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if epoch_iter % 20 == 0:\n",
        "      #show_images_with_masks_and_prediction(images, labels, outputs)\n",
        "\n",
        "      loss_avg = running_loss / 20\n",
        "      #print('[%d, %5d] Running loss: %.3f' % (epoch + 1, epoch_iter + 1, loss_avg))\n",
        "      # (10 * x + y) / 10\n",
        "      i_num = (epoch + 1) + ((epoch_iter + 1) / total_iter)\n",
        "      tb.add_scalar('RunningLoss', loss_avg, i_num)\n",
        "      running_loss = 0.0\n",
        "\n",
        "  loss = loss_per_epoch / epoch_batch_count\n",
        "  tb.add_scalar('Loss', loss, epoch + 1)\n",
        "  print('*********** [%d] Loss per epoch: %.3f' %(epoch + 1, loss))\n",
        "\n",
        "  if loss <= best_train_loss:\n",
        "    best_train_loss = loss\n",
        "\n",
        "  # Validation Iterations\n",
        "  running_loss_valid = 0.0\n",
        "  loss_per_epoch_valid = 0.0\n",
        "  epoch_batch_count_valid = 0\n",
        "  loss_valid = 100\n",
        "  if NETWORK_OPTIONS_OBJ.USE_VAL_SET:\n",
        "    with torch.no_grad():\n",
        "      for epoch_iter, (images, labels) in tqdm(enumerate(valid_loader, 1), total=len(valid_loader)):\n",
        "        # get batch\n",
        "        inputs = images.float().to(device)\n",
        "        masks = labels.to(device)\n",
        "        # assert input.size() == target.size()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # update losses for epoch\n",
        "        loss_per_epoch_valid += loss.item()\n",
        "        epoch_batch_count_valid += 1\n",
        "        running_loss_valid += loss.item()\n",
        "\n",
        "        if epoch_iter % 20 == 0:\n",
        "          loss_avg = running_loss_valid / 20\n",
        "          print('[%d, %5d] Running loss Validation: %.3f' % (epoch + 1, epoch_iter + 1, loss_avg))\n",
        "          # (10 * x + y) / 10\n",
        "          i_num = (10 * (epoch + 1) + (epoch_iter + 1)) - 10\n",
        "          tb.add_scalar('RunningLossValidation', loss_avg, i_num)\n",
        "          running_loss_valid = 0.0\n",
        "\n",
        "    loss_valid = loss_per_epoch_valid / epoch_batch_count_valid\n",
        "    tb.add_scalar('LossValidation', loss_valid, epoch + 1)\n",
        "    print('*********** [%d] Validation Loss per epoch: %.3f' %(epoch + 1, loss_valid))\n",
        "\n",
        "\n",
        "  is_best = False\n",
        "  if loss_valid < best_valid_loss:\n",
        "    best_valid_loss = loss_valid\n",
        "    is_best = True\n",
        "\n",
        "  create_checkpoint(run_name, model, optimizer, is_best, epoch, NETWORK_OPTIONS, loss, loss_valid, best_valid_loss)\n",
        "\n",
        "  # reset loss per epoch\n",
        "  loss_per_epoch = 0.0\n",
        "  epoch_batch_count = 0\n",
        "  # reset validation loss per epoch\n",
        "  loss_per_epoch_valid = 0.0\n",
        "  epoch_batch_count_valid = 0\n",
        "  \n",
        "  tb.flush()\n",
        "\n",
        "  \n",
        "  # Update the model learning rate\n",
        "  # model.update_learning_rate()\n",
        "tb.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDMAoWUJdChc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate loss on test set\n",
        "loss_test = 0.0\n",
        "batch_count_test = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for epoch_iter, (images, labels) in tqdm(enumerate(test_loader, 1), total=len(test_loader)):\n",
        "    # get batch\n",
        "    inputs = images.float().to(device)\n",
        "    masks = labels.to(device)\n",
        "    # assert input.size() == target.size()\n",
        "\n",
        "    # forward\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, masks)\n",
        "\n",
        "    # update losses for epoch\n",
        "    loss_test += loss.item()\n",
        "    batch_count_test += 1\n",
        "\n",
        "loss_test = loss_test / batch_count_test\n",
        "tb.add_scalar('LossTest', loss_test, loss_test)\n",
        "print('\\n Loss on Test Set: %.3f' %(loss_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}